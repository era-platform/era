== Dependent type notation in the staged calculus of structures ==

Where most dependent type theories have Pi and Sigma types with
notations like these...

\Pi (a : A) -> B{a}
\Sigma (a : A) -> B{a}

...the calculus of structures has involutive negation that follows the
laws of linear logic, so we must understand how such negation affects
these constructs.

The quantifiers formulated in "Redesigning the CLF type theory" by
Anders Schack-Nielsen 2009
<http://www.logosphere.org/~celf/download/clf.pdf> are more useful
from a linear logic point of view, since they allow us to make the
exponentials explicit:

\PiHat (a : !A) -> B{a}
\SigmaHat (a : !A) -> B{a}

As discussed in that paper, PiHat in the non-dependent case is like
(!A -o B), and SigmaHat is like (!A * B) where * is multiplicative
conjunction. So these are two multiplicative connectives, and we're
going to try to treat them as duals of each other.

Here's a notation that emphasizes their duality:

D@s[-rst.a : -rst.!rs.A].B{a}
D@s(a : !rs.A).B{a}

This way we get this De Morgan's equivalence and its companion:

D@s[-rst.a : -rst.A].-rst.B{a}
-rst.D@s(a : A).B{a}

The paper talks about how B{a} may only depend on intuitionistic
variables of the pattern `a`. (The paper uses a detailed pattern
language, which we'll somewhat imitate in a moment.) If the second
type gets to depend on !-modal parts of the first value, why not let
the first type depend on !-modal or ?-modal parts of the second value
as well? Let's not worry about exactly what modalities we permit, and
let's remix the notation again:

-- originals
\PiHat (a : !A) -> B{a}
\SigmaHat (a : !A) -> B{a}

-- our new notation
@s[-rst.a _]:@s[-rst.!rs.A B{a}]
@s(a _):@s(!rs.A B{a})

-- De Morgan
@s[-rst.a -rst.b]:@s[-rst.A{b} -rst.B{a}]
-rst.@s(a b):@s(A{b} B{a})

(TODO: See if back-and-forth dependency turns out to be well-founded.)

If back-and-forth dependency turns out to be well-founded, then it'll
give us commutativity and a more thorough way to express rules like
the switch rule in their full dependent type expressiveness:

@s(@s[r u] t):@s(@s[R{u}{t} U{r}{t}] T{r}{u})
@s[@s(r t) u]:@s[@s(R{u}{t} T{r}{u}) U{r}{t}]

Notice that T depends on u and U depends on t, even though this rule
switches the nesting of these bindings. If we weren't using a pattern
language here, we would have trouble describing this.

With examples like this in mind, it turns out we might just want to
tag specific substructures with variable names (or, in general,
patterns) and use them throughout the currently described rule:

@s(@s[r:R{u}{t} u:U{r}{t}] t:T{r}{u})
@s[@s(r:R{u}{t} t:T{r}{u}) u:U{r}{t}]

At this point, it's easy to take the variables for granted and just
specify the rule the old-fashioned way:

@s(@s[R U] T)
@s[@s(R T) U]

It seems just as easy to work backwards and get analogues for all the
other rules. At the same time, we obtain curious connectives for
dependently typed additives.

One potential obstacle to this system is the use of variables. Each
connective introduces variables with scoping rules that may be subtly
different, so letting them all get mixed up in one environment could
reduce clarity. We'll see this pretty soon once we have at least one
notation that lets a type depend on a term.

Case in point: Let's start handling observational equality (as in
"Observational Equality, Now!" Altenkirch, McBride, Swierstra 2007,
and also as in era-sequents.txt). We need types (propositions) for
type equality and value equality:

A=B
<a A == b B>

The paper's equality of dependent pair types is defined like so. We
encode the propositional "and" connective as @s(...), since the
coercion that motivates this definition uses both operands exactly
once apiece:

@s(a:!rs.A B{a})=@s(c:!rs.C D{c})
- is defined as -
@s(A=C
   @s[-rst.a:!rs.A -rst.c:!rs.C -rst.!rs.<a !rs.A == c !rs.C> B{a}=D{c}])

If we remove the exponentials and allow bidirectional dependency, we
may be looking at something like this instead:

@s(a:A{b} b:B{a})=@s(c:C{d} d:D{c})
- is defined as -
@s(@s[-rst.b:B{a} -rst.d:D{c} -rst.<b B{a} == d D{c}> A{b}=C{d}]
   @s[-rst.a:A{b} -rst.c:C{d} -rst.<a A{b} == c C{d}> B{a}=D{c}])

This interpretation doesn't quite seem to get us within a comfortable
proximity to of the paper's equality, but it's not too distant either:

@s(a:!rs.A B{a})=@s(c:!rs.C D{c})
- is defined as -
@s(@s[-rst.b:B{a} -rst.d:D{c} -rst.<b B{a} == d D{c}> !rs.A=!rs.C]
   @s[-rst.a:!rs.A -rst.c:!rs.C -rst.<a !rs.A == c !rs.C> B{a}=D{c}])

If we rewrite it a bit, we can at least get something less cluttered:

@s(a:A{b} b:B{a})=@s(c:C{d} d:D{c})
- is defined as -
-rst.@s[@s(b:B{a} d:D{c} <b B{a} == d D{c}> -rst.A{b}=C{d})
        @s(a:A{b} c:C{d} <a A{b} == c C{d}> -rst.B{a}=D{c})]

This suggests we may want to work with the dual of type equality in
the first place... assuming we can make sense of such a thing.

We've sort of put the cart before the horse here, because we still
have neither a term language nor any desired rules for how equality
should be useful. Perhaps the rule we're looking for is something like
this:

A=B
@s[-rst.a:A @s(b:B <a A == b B>)]

TODO: See if the above definition of sigma equality allows us to
derive this rule.

To start us off on a term language, here's a similar attempt to adapt
the definition of sigma equality for values rather than types:

<x @s(a:A{b} b:B{a}) == y @s(c:C{d} d:D{c})>
@s(<#fst.x A{b} == #fst.y C{d}> <#snd.x B{a} == #snd.y D{c}>)

We have two problems: Where are a, b, c, and d defined in the
consequence of that rule? How can we be comfortable with the semantics
of #fst and #snd when they impose an ordering on the contents of an
otherwise unordered connective like dependent @s(...)?

For that matter, we've had another problem since we started
investigating equality on @s(...). We've been treating the two sides
of the equality as though they have the same order too.

Let's solve all three of these problems by separating variables and
element labels into two concepts, "x$" and "x:". We'll use "#x:" to
access label "x:" instead of using #fst or #snd.

<x w$@s(m:a$A{b} n:b$B{a}) == y z$@s(m:c$C{d} n:d$D{c})>
@s(<#m:.x a$A{b} == #m:.y c$C{d}> <#n:.x b$B{a} == #n:.y d$D{c}>)

Note that in the process of applying this rule, any uses of variables
"w" and "z" would need to be substituted with $s(m:a n:b) and
$s(m:c n:d).

At this point, the use of patterns instead of variables has been
pretty much forgotten.

---

What follows is an attempt to express type constructors as rules in
the calculus of structures itself. They're metacircular and, for now,
very incomplete.


(Type Type)
-- Make any non-dependent [_ _] or (_ _)
Type

()
-- Make [] or ()
Type

[-.A Type]
-- Make any non-dependent [_ _ ...] or (_ _ ...) with any index type
-- that exists before this construction step, which isn't quite as
-- expressive as dependent (a$_ _{a}) or [-.a$_ _{a}] but comes close
[(A []) Type]

[-.A Type]
-- Same as above, but using clearer linear implications
[-.[-.A ()] Type]

[A Type]
-- Same as above, but if locality isn't a concern (and maybe we didn't
-- have locality anyway, if we needed A to appear in a negative
-- position in the premiss and a positive position in the conclusion)
Type

-- NOTE: This rule is especially experimental! It might not be
-- correct in any way.
[-.(m:-.x1$x n:[m:y1$y n:z1$z])
  (m:-.(x2$x <x1 x == x2 x> x$Type)
    n:[m:(y2$y <y1 y == y2 y> y$Type)
        n:(z2$z <z1 z == z2 z> z$Type)])]
-- Make the dependent structure
-- (m:-.x$_{y}{z} n:[m:y$_{x}{z} n:z$_{x}{y}])
Type


That last rule is meant to suggest a whole family of rules, but what
are the rules to construct the family? How about...

Define the notation <x1$ -- x2> to mean this:

(x1$(type:Type val:#type:.x1) <#type:.x1 = #type:.x2>
  <#val:.x1 #type:.x1 == #val:.x2 #type:.x2>)

[-.(<x1$ -- x2> <y1$ -- y2>) (<x2$ -- x1> <y2$ -- y1>)]
-- Make any dependent (x$_{y} y$_{x}).
Type

[-.[<x1$ -- x2> <y1$ -- y2>] [<x2$ -- x1> <y2$ -- y1>]]
-- Make any dependent [x$_{y} y$_{x}].
Type

()
-- Lift the inference rule
-- "([a$A{b}{c} b$B{a}{c}] c$C{a}{b}) |-
-- [(a$A{b}{c} c$C{a}{b}) b$B{a}{c}]"
[-.([<a1$ -- a> <b1$ -- b>] <c1$ -- c>)
  [(<a2$ -- a> <c2$ -- c>) <b2$ -- b>]]

()
-- Lift the inference rule "() |- [-.A A]"
[-.() [-.<a1$ -- a2> <a2$ -- a1>]]

This technique seems resilient to changes in the meaning of
<x1$ -- x2>, <_ == _>, <_ = _>, and "Type". Let's do this over again
with a universe hierarchy.

Meanwhile, oops, the above were actually non-dependent inference
rules. Dependent ones would have operated on values of dependent
structure types. Let's use the notation "A=B" to mean that a structure
is represented by the type variable A now, but that its value will be
used as a type written "B".

Define the notation <x1$ -- x2 :: u> to mean this:

(x1$(type:t$u val:v$t) <t u == #type:.x2 u>
  <v t == #val:.x2 #type:.x2>)

[-.(<x1$ -- x2 :: U=a$A{a}{b}> <y1$ -- y2 :: U=b$B{a}{b}>)
  (<x2$ -- x1 :: U> <y2$ -- y1 :: U>)]
--
U=(a$A{a}{b} b$B{a}{b})

[-.[<x1$ -- x2 :: U=a$A{a}{b}> <y1$ -- y2 :: U=b$B{a}{b}>]
  [<x2$ -- x1 :: U> <y2$ -- y1 :: U>]]
--
U=[a$A{a}{b} b$B{a}{b}]

UParent
--
[-.(u$UParent
     <x1$ -- x :: u=([a$A{a}{b}{c} b$B{a}{b}{c}] c$C{a}{b}{c})>)
  <x2$ -- x :: u=[(a$A{a}{b}{c} c$C{a}{b}{c}) b$B{a}{b}{c}]>]

UParent
--
[-.u$UParent [-.<a1$ -- a2 :: u=A> <a2$ -- a1 :: u=A>]]

Now for new stuff:

TODO: Figure out how to construct and deconstruct values of dependent
structure types. Here's an incomplete attempt:
  UParent
  --
  [-.(u$UParent <x3$ -- x1 :: u=a$A{a}{b}> <x4$ -- x2 :: u=b$B{a}{b}>)
    <x5$ -- x6 :: u=(a$A{a}{b} b$B{a}{b})>]

T
-- Extract reflexivity
[-.x$T <x T == x T>]

(U1 U2)
-- Extract coercion
[-.(t1$U1 t2$U2 <t1 U1 == t2 U2> x1$t1) (x2$t2 <x1 t1 == x2 t2>)]

U
-- Extract cumulativity (remembering that == is actually coercive
-- subtyping rather than coercive equality, despite the notation)
[-.(t$U x1$t) (x2$U <x1 t == x2 U>)]
--
-- TODO: Actually this might be incorrect, since it can transport
-- values to (presumably empty) types. Adding another layer (xx$x1)
-- would make it impossible to transport empty types. Perhaps we'll
-- need to reintroduce the "Type" constant if the empty type
-- interpretation doesn't work out.
