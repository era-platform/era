Profinite distributive lattices


Stephen Dolan's thesis, "Algebraic Subtyping" (available at https://www.cl.cam.ac.uk/~sd601/mlsub/) defines a type system called MLsub, which accomplishes subtyping with principal type inference by having the types form a profinite distributive lattice. Some of the inferred types are fixpoint types. For a type formula to have a unique least pre-fixed point and a unique post-fixed point, the uses of its type variable must be covariant. For the two to coincide in a unique fixed point, the uses must be guarded by type constructors other than union and intersection. For ease of type inference, MLsub restricts the use of union to positive positions and intersection to negative ones, which suffices to infer types for all MLsub program terms.

Here we try to extend that approach for dependently typed programming. As in MLsub, our types form a profinite distributive lattice for subtyping, so to be able to represent type universes as types and morphisms between them as functions, our functions (dependent products) are themselves homomorphisms of profinite distributive lattices, and every type must therefore be a profinite distributive lattice. This means all our types are technically nonempty (having at least a bottom value and a top value), and the bottom type itself represents a lattice where the bottom value and top value can't be distinguished (and may in fact coincide). While it's unusual for a bottom type to be inhabited, arguments by absurdity still make sense since this trivial lattice is trivially a substructure of every other type's lattice.

We not only provide depdendent products but also dependent sums, dependent unions, and dependent intersections. These are likely to complicate type inference (since MLsub's automaton representation of types takes particular advantage of binary unions and intersections, not indexed ones), but other dependent type systems have complicated their type inference in ways that manage to still be pretty practical in most cases, so some compromises in type inference are acceptable here.

As with MLsub, we want subtyping proofs to be inferable without ambiguity. So we at least require that if one type is a subtype of another, that there is at least one coercion that is least surprising. MLsub goes further and requires that there are no two ways that one type is a subtype of another (hence building their subtyping structure out of a preorder rather than a general category), and we may wnat to be strict like that too. But either way, the category of subtyping relationships doesn't seem to be cartesian closed; when (a * b) is the product in that category (type intersection) and (a -> b) means a proof that `a` is a subtype of `b`, it doesn't make sense for ((a * b) -> c) to imply (a -> (b -> c)). So, if we make any kind of dependent type theory here, the way we give semantics to contexts and formulas must be in a more general cartesian closed category that has subtyping morphisms as a special case.

To avoid exposing implementation details of the profinite distributive lattices, it would be best for our morphisms to respect the profinite distributive lattice structure. For instance, the formulas of our type theory should be monotonic in the variables they use, and they should preserve joins and meets. (This should mean it's admissible to manipulate expressions by rewriting their subexpressions, the way the calculus of structures does.) Certain type constructors, such as function types, are usually contravariant when they're presented, but it seems like we can make this work by treating certain types as the duals of other types.

Datafun (http://www.rntz.net/datafun/) is an existing type theory where certain variables are used in a monotonic way, so it may be a good reference to consult.

At some point, we might be able to support a lambda-like syntax for subtyping relations. We have a couple of interesting choices if we do:

  - Subtyping relations are like functions from a subtype to a supertype, with the strict limitation that they can only return something that's an extension to (or the same as) their input. With careful typing rules, we might be able to enforce that limitation.

  - In cubical type theory (https://www.math.ias.edu/~amortberg/papers/cubicaltt.pdf, https://github.com/mortberg/cubicaltt), a lambda-like syntax can be used to prove propositional equalities (well, paths). The input in this case is a lattice element representing a position along an interval, and the output is the value occurring at that point, defining a full path of intermediate stages between the value at one endpoint and the value at another. Since we're going with the idea that two types have a subtyping relationship in at most one way, we probably don't need to track the content of a subtyping path this comprehensively yet, but it's an interesting precedent to keep in mind.

  - At some point we may like to observe that even if the subtyping relation has an implementation that's uniquely determined by its types, we can still treat monotonic function types as being sugar for a special case of subtyping propositions by using unions and intersections to hide the implementation details:

      (<=<= a : A. B a)
      (A <= B) means (<=<= ignored : A. B)
      FunctionComputeDetails
      FunctionCompute D A
      FunctionComputeResult D A
      FunctionComputeInit X
      (** a : A. B a) means (|| da : FunctionComputeDetails. || db : FunctionComputeDetails. <=<= a : FunctionCompute da A. FunctionCompute db (B (FunctionComputeResult da a)))
      (F X) means (^^ db : FunctionComputeDetails. FunctionComputeResult db (F (FunctionComputeInit X)))

    However, monotonic functions are so intrinsic to the way this type theory deals with its context formation and binder syntaxes that it's going to be easier to treat them as being built in.

    One place this would become tempting again is when pursuing meaning-preserving modularity. Instead of using all the rules of union types (which are weak existentials), we'll want union types to be sugar for exporting and importing values under obscure names. Then another module with access to the author's secrets can come along and say something else about the same obscure names as a way to safely extend the interface of the existing module. Still, in that case we might decide functions are sugar for something simpler:
    
      (**+ a : A. B a)
      (** a : A. B a) means (**+ a : A. || b : B a. b)
    
    ...Although hmm, I guess (|| b : B a. b) would always simplify to Top.

First, let's draw up a syntax where the type constructors *don't* have to be covariant:

Let: (Let x = A, y = B in C x y)
Least pre-fixed point type: (Fix| x. A x)
Greatest post-fixed point type: (Fix^ x. A x)
Unit type: 1
Unit value introduction: Unit
Boolean type: 2
Boolean value introduction: True and False
Boolean value elimination: (If C A B), with (IsTrue C) meaning (If C 1 Bot)
Bottom type: Bot
Bottom value elimination: Absurd B
Subtyping proposition: (A <= B). Note that 1 is isomorphic to (2 <= 2), along with many other things.
Indexed profinite distributive lattice product type: (** a : A. B a), with (A * B) meaning (** i : 2. If i A B), (A -> B) meaning (** a : A. B), and  (**: A. B) also meaning (** a : A. B)
Indexed profinite distributive lattice product value introduction: (x \* A x)
Indexed profinite distributive lattice product value elimination: (F X)
Indexed profinite distributive lattice sum type: (++ a : A. B a), with (A + B) meaning (++ i : 2. If i A B). Note that Bot is isomorphic to (++ a : Bot. B a) and (A * B) is isomorphic to (++ a : A. B).
Indexed profinite distributive lattice sum value introduction: (A \+ B)
Indexed profinite distributive lattice sum value elimination: (Fst AB) and (Snd AB)
Indexed subtyping sum (join/union) type: (|| a : A. B a), with (A | B) meaning (|| i : 2. If i A B). Note that Bot is isomorphic to (|| a : Bot. B a).
Indexed subtyping product (meet/intersection) type: (^^ a : A. B a), with (A ^ B) meaning (^^ i : 2. If i A B) and Top meaning (^^ a : Bot. B a)
Type of types: Type

Now we can systematically define a set of fully covariant type constructors and their fully covariant duals, by using the same syntaxes and adding negation as needed:

Let x = A, y = B in C x y
  (self-dual)
Fix| x. A x
Fix^ x. A x
  (duals with each other)
1
-1
Unit
-Unit
2
-2
True
-True
False
-False
If C A B
-If -C -A -B
Bot
-Bot
Absurd B
-Absurd -B
-A <= B
-(A <= -B)
** a : -A. B a
-(** a : A. -(B a))
x \* A x
-(x \* -(A x))
F X
-(-F -X)
++ a : A. B a
-(++ a : -A. -(B a))
A \+ B
-(-A \+ -B)
Fst AB
-Fst -AB
Snd AB
-Snd -AB
|| a : A. B a
^^ a : -A. B a
  (duals with each other)
Type
-Type

We do likewise for all the abbreviations, such as (-a -> b) and its dual -(a -> -b).

Some of these negative types might coincide nicely with positive types. In particular, we've identified the fixpoints and the union/intersection types as duals of each other because those types only come and go due to type inference and explicit type ascriptions, where the subtyping relations between "negative types" and "positive types" don't need to be segregated. It's tempting to compare this choice to MLsub's polar types, which have the property that unions are only allowed in positive positions and intersections are only allowed in negative ones.

When regarding those type constructors as functions, many of their types can be simply `Type`, (--Type -> --Type -> Type), (** x : --Type. --(x -> --Type) -> Type), or perhaps variations thereof that take multiple type universes into account. For instance, if we had a cumulative type hierarchy, we might like a binary type constructor to have a family of types like (--U -> --V -> (U | V)), for type universes U and V and their negations -U and -V. A binder is usually of type (** x : --U. --(** a : x. --(V x a)) -> U | || a : x. V x a) for every type universe U and its negation -U and every family of type universes (V x a) and their negations -(V x a). (For some binders that expect a dual-type, the first (x : --U) may be (x : -U) instead.)

So far, everything we've done has respected covariance of its bound variables. However, technically we can use bound variables in contravariant ways, as we'll show now on the topic of polymorphism and equality propositions.

A traditional type system would type the polymorphic identity function like so:

  id : || a : Type. a -> a
  id x = x

However, that uses the variable `a` both covariantly and contravariantly, and it uses the partially contravariant type constructor (a -> b) which we don't have in our system. We need to use the type constructor (-a -> b), so we need access to a negative counterpart to `a` for use in the function's domain. Fortunately, we can get access to that negative counterpart by being polymorphic over another variable and a subtyping proposition between the two:

  id : || pa : Type. || na : -Type. || ignored : (-na <= pa). -na -> pa
  id x = x

This type signature reads "For all types pa, for all dual-types na, for all proofs that -na is a subtype of pa, here's a monotonic function from -na to pa."

That pattern will come in handy a lot. We can abbreviate it:

(~~ nx, sx = -X : A. B nx sx) means (|| nx : A. || sx : (-nx <= X). B nx sx)
(~~ nx = -X : A. B nx sx) means (~~ nx, ignored = -X : A. B nx)

(** px, nx, sx : -NA. B px nx sx) means (** px : -NA. ~~ nx, sx = -px : NA. B px nx sx)
(** px, nx : -NA. B px nx) means (** px, nx, ignored : -NA. B px nx)
(^^ px, nx, sx : -NA. B px nx sx) means (^^ px : -NA. ~~ nx, sx = -px : NA. B px nx sx)
(^^ px, nx : -NA. B px nx) means (^^ px, nx, ignored : -NA. B px nx)
(++ px, nx, sx : PA, NA. B px nx sx) means (++ px : PA. ~~ nx, sx = -px : NA. B px nx sx)
(++ px, nx : PA, NA. B px nx) means (++ px, nx, ignored : PA, NA. B px nx)
(|| px, nx, sx : PA, NA. B px nx sx) means (|| px : PA. ~~ nx, sx = -px : NA. B px nx sx)
(|| px, nx : PA, NA. B px nx) means (|| px, nx, ignored : PA, NA. B px nx)

But the covariant binders require so much duplication of code for this (to fill in both the NA and PA slots with complementary types) that we might as well go further and use the abbreviations (+## ...) and (-## ...) to use a non-polar expression in a polar context and the abbreviations (##+ ...) and (##- ...) to use a polar expression in a non-polar context. If you like, imagine that in a non-polar expression, instead of having each operator build a single expression, we have it build a data structure of two expressions and a subtyping proposition between them. Then we access just one side of that data structure on our way out.

We can define the equality proposition (A = B) as syntax sugar using the same technique. Up to isomorphism, we can have (A = B) stand for (~~ na = -A : -Type. ~~ nb = -B : -Type. ((-na <= B) * (-nb <= A))), or more conveniently (+## ((A <= B) * (B <= A))).

The universe-conscious type of the binder (^^ a : -A. B a) can be (** x : -U. --(** a : x. --(V x a)) -> U | || a : x. V x a), as described above. However, it may also be more precise, at least when the intersection is nonempty, if we use an intersection in the last part instead of a union: (** x : -U. --(** a : x. --(V x a)) -> U | ^^ a : x. V x a).

There's a distinct risk of Girard's paradox in this system because of the bounded lattice structure. Namely, Bot and Top are inhabitants of all types. We could probably construct Girard's paradox using (Top : Top), but we don't need to because we already have a term of type Bot, namely (Bot : Bot) itself. So this is currently a very sketchy system for mathematics!

Perhaps something we can do to avoid paradoxes is to keep all these types as potential type inference results, but limit the type annotations the programmer can actually write like so:

  - Prohibit the literal use of `Bot` and `Top`.

  - Prohibit the literal use of (Fix| a. B a), (Fix^ a. B a), (|| a : A. B a), and (^^ a : -A. B a) where (B a) makes unguarded use of `a`.

This effectively makes sure a type is codata (always making progress with a constructor, even if it never ends) in a way that ensures that its value is codata, since every type constructor ensures there are one or more value constructors at the root of any given value of the type. There may still be some types that are isomorphic to `Bot`, but they're not equal to it.

Considering that way of thinking about it, any program that includes user-defined types will constitute one giant coinductive-corecursive definition of the type of types.

I imagine a module is a monotonic function from (<a supply of names>, <a definition state which is free over an unknown set of different names>) to <a definition state which is an extension of the original and is free over the union of the name supplies>. Instead of pulling type constructors out of nowhere, they come out of the original definition state. Since we have access to a supply of names the original definition state didn't account for, we potentially have different Top and Bot values than it could account for as well.



\= ===== An attempt at describing the type of a module ===============


+##
** typeNil : Kind.
** typeNext : (Kind -> Kind).
** union :
  ( || u : typeNil.
    || vRest : typeNil.
    ** x : u.
    || v : (x -> typeNext vRest).
    **: (** a : x. v a).
    (u | || a : x. v a)).
\= That type again with fully polarized types:
\=
\= ##+
\= ** union :
\=   -( ^^ u, nu : -ntypeNil.
\=      ^^ vRest, nvRest : -ntypeNil.
\=      -** x, nx : u, nu.
\=      -^^ v, nv : --(x -> --(-ntypeNext -nvRest)).
\=      -**: (** a : -nx. v a).
\=      -(nu ^ ^^ a, na : -nx. -(-nv -na))).
\= +##
...
(TODO: Add the rest of the syntaxes as arguments here.)




\= ===== An attempt at inductive and coinductive types ===============


We can build inductive and coinductive definitions in the following style:

(** elem : --U.
  ++ list, nlist : U, -U.
  ** p, np : --(list -> --U).
  **:
    --(** goodList, ngoodList : U, -U.
        --**: (-ngoodList <= list).
        --**: (** x : -ngoodList. p x).
        --** x, nx : (1 + (elem * goodList)), -(-1 + -(-elem * -goodList)).
        --(-np -nx)).
  (** x : -nlist. p x))

(** elem, nelem : --U.
  ++ stream, nstream : U, -U.
  ** p, np : --(stream -> --U).
  **:
    --(** goodStream, ngoodStream : U, -U.
        --**: (-nstream <= goodStream).
        --**: (** x : --(--1 + --(-nelem * -ngoodStream)). p x).
        --** x, nx : goodStream, ngoodStream. --(-np -nx)).
  (** x : -nstream. p x))

This suggests the following abbreviations:

(Ind a : U, NU. B a, NB a) means
  (++ a, na : U, NU.
    ** p, np : --(a -> -NU).
    **:
      --(** goodA, ngoodA : U, NU.
          --**: (-ngoodA <= a).
          --**: (** x : -ngoodA. p x).
          --** x, nx : B goodA, NB goodA. --(-np -nx)).
    (** x : -na. p x))
  aka
  +##
  ++ a : U.
  ** p : (a -> U).
  **:
    (** goodA : U. **: (goodA <= a).
      (** x : goodA. p x) ->
      (** x : B goodA. p x)).
  (** x : a. p x)

(Coind a : U, NU. NB a) means
  (++ a, na : U, NU.
    ** p, np : --(a -> -NU).
    **:
      --(** goodA, ngoodA : U.
          --**: (-na <= goodA).
          --**: (** x : -(NB goodA). p x).
          --** x, nx : goodA, ngoodA. --(-np -nx)).
    (** x : -na. p x))
  aka
  +##
  ++ a : U.
  ** p : (a -> U).
  **:
    (** goodA : U. **: (a <= goodA).
      (** x : ##- (NB goodA). p x) ->
      (** x : goodA. p x)).
  (** x : a. p x)

(TODO: Also explore induction-recursion and coinduction-corecursion.)



(TODO: The rest of the sections below don't account for recent changes we've made to the above syntax. In particular, we didn't write the below with awareness of polarized types. Make changes to the following sections to bring them up to date. The sequent calculus will be much easier to specify now that we depend on every variable monotonically instead of trying to write lambdas to define subtyping propositions.)



\= ===== An attempt at presenting this system with rewrite rules =====


Here are some laws these primitives might follow:

^^ a : A. B
=
B

^^ a : A. B a
<=
B x

^^ a : A. ^^ b : B. C a b
<=
^^ b : B. ^^ a : A. C a b

^^ a : A. ^^ b : B a. C a b
=
^^ ab : (++ a : A. B a). C (Fst ab) (Snd ab)

^^ a : A. ^^ b : B a. C b
=
^^ b : (|| a : A. B a). C b

The dual of that last law also uses || in the type of `b`, not ^^:

|| b : (|| a : A. B a). C b
=
|| a : A. || b : B a. C b


The distributive law for ^ over | (and dually for | over ^) involves the use of a choice function:

^^ a : A. || b : B a. C a b
<=
|| bf : (** a : A. B a). ^^ a : A. C a (bf a)

(TODO: The choice function should probably be intuitionistic, not monotonic (as we have it now).)

Note that the choice function is needed even in the independent case:

^^ a : A. || b : B. C a b
<=
|| bf : (A -> B). ^^ a : A. C a (bf a)

The need for this can be seen in particular when the `a` and `b` indexes are the boolean type. In this case, the distributive law results in a four-way | rather than merely a two-way one:

(C False False | C False True) ^ (C True False | C True True)
<=
(C False False ^ C True False) | (C False False ^ C True True) | (C False True ^ C True False) | (C False True ^ C True True)


(TODO: Make sure we have enough laws to prove commutativity, associativity, and absorption of the lattice operations.)



\= ===== An attempt at presenting this system with sequents ==========


(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases |- b : B
---
env; mono; aliases |- b : ^^ a : A. B

env; mono; aliases |- b : || a : A. B
---
(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases |- b : B

env; mono, a : A; aliases, A |- b : B
---
env; mono; aliases |- a \<= b : <=<= a : A. B

env; mono; <=<= a : A. B |- f : <=<= a : A. B
env; mono; aliases, C |- a2 : A
---
env; mono; aliases, C |- Call<= f a2 : B[a2/a]

env; mono; aliases |- f : <=<= a : A. B
---
env; mono; aliases |- a2 \<= Call<= f a2 : <=<= a : A. B

env; mono; aliases, A |- b : B
(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases, C |- d : D
---
env; mono; aliases, ++ a : A. C |- b \+ d : ++ a : B. D

env; mono; aliases, ++ a : A. B |- c : ++ d : D. E
---
env; mono; aliases, A |- Fst c : D

env; mono; aliases, ++ a : A. B |- c : ++ d : D. E
---
env; mono; aliases, B[(Fst c)/a] |- Snd c : E[(Fst c)/d]
