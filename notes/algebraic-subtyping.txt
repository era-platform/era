Dependent Typing with Algebraic Subtyping


Stephen Dolan's thesis, "Algebraic Subtyping" (available at https://www.cl.cam.ac.uk/~sd601/mlsub/) defines a type system called MLsub, which accomplishes subtyping with principal type inference by having the types form a profinite distributive lattice, particularly one that can be constructed as a sum with one term for each type system feature, and which for each set of type system features can be extended to a free algebra over a set of polymorphic type variables.

The algebra in the paper is constructed as an initial algebra like so, where L^TB means another distributive lattice constructed by adding an additional top and an additional bottom element to L, L^T just adds a top, and L^op is the dual of L:

    Bool(A) = 1
    Func(A) = A^op * A
    Record(A) = A^T * A^T * A^T * A^T * A^T * ... (one term for each record key label)
    F(A) = Bool(A)^TB + Func(A)^TB + Record(A)^TB

The free distributive lattice over a set of generators V is a |V|-way sum of 1^TB, and the paper shows that these extra terms can be added to the above technique to make a free F-algebra over V.

Intuitively, we can think of each term L^TB as admitting that there's more to the overall type system than the types that exist in the type system feature L itself, which then makes it possible to compose two features nicely.

In MLsub, some of the inferred types are fixpoint types, so the paper addresses the question of when fixpoints are defined. For a type formula to have a unique least pre-fixed point and a unique post-fixed point, the uses of its type variable must be covariant. For the two to coincide in a unique fixed point, the uses must be guarded by type constructors other than union and intersection. For ease of type inference, MLsub restricts the use of union to positive positions and intersection to negative ones, which suffices to support type inference for all MLsub program terms.

Here we try to extend MLsub's approach with the additional goal of dependently typed programming. As in MLsub, our types form a profinite distributive lattice for subtyping. Because we're going to do dependently typed programming, we may deal with value types that act as "type universes" (such as (Type : Type) or something more well-founded), and we must respect the lattice structure of these types or else we'll disrupt the substitutability of subtypes at the call site. Hence, all our functions must be morphisms that respect the structure of their type. For now we'll even impose the more specific design guideline that we're working in the category of profinite distributive lattices; hence, all the types of are lattices, and all functions must preserve lattice structure.

Michael Arntzenius's Datafun (http://www.rntz.net/datafun/) is an existing type theory where certain variables are used in a monotonic way, so it may be a good reference to consult.

(When the types we're dealing with are free distributive lattices, respecting their lattice structure is easy: An arbitrary function from generators of A to the elements of B can be uniquely extended to a structure-preserving function from all elements of A to elements of B, so we only need to deal with the generators. It may turn out most of our types fit these conditions, since the initial algebra approach above resembles the structure of a free distributive lattice, but what we're doing doesn't yet require all types to be this way. In fact, we will drop this assumption once we get to building a type universe hierarchy.)

Since all our types are lattices (and we're using the definition Dolan does where all lattices are bounded), this means all our types are technically nonempty: Every type contains a bottom value and a top value. The bottom type contains its bottom and top values and nothing else. While it's unusual for a bottom type to be inhabited, arguments by absurdity still make sense since there's a unique lattice-structure-preserving map from the bottom type to any other type.

In our dependent type theory, we will not only provide depdendent products but also dependent sums, dependent unions (existential types), and dependent intersections (universal types). These are likely to complicate type inference (since MLsub's automaton representation of types takes particular advantage of binary unions and intersections, not indexed ones), but the type inference of other dependent type systems in practice today tends to have limitations already, and they manage to still be practical in most cases, so some compromises in type inference are acceptable here.

As with MLsub, we want subtyping proofs to be inferable without ambiguity. To get this, we at least require that if one type is a subtype of another, that there is at least one coercion that is least surprising. MLsub goes further and requires that there are no two ways that one type is a subtype of another (hence building their subtyping structure out of a preorder rather than a general category), and we may wnat to be strict like that too.

When determining what category to work in, one tempting option was to force every morphism to be a subtyping coercion. But subtyping coercions don't seem to be cartesian closed; when (a * b) is the product in that category (type intersection) and (a -> b) means a proof that `a` is a subtype of `b`, it doesn't make sense for ((a * b) -> c) to imply (a -> (b -> c)). So, we can't do much type theory directly in that category, as it doesn't let us write a nested functions as a formula over a nested environment. Nevertheless, at some point we might be able to support a lambda-like syntax for proofs of subtyping. We have a couple of interesting things to consider if we do:

  - Subtyping coercions are like functions from a subtype to a supertype, with the strict limitation that they can only return something that's an extension to (or the same as) their input. With careful typing rules, we might be able to enforce that limitation.

  - In cubical type theory (https://www.math.ias.edu/~amortberg/papers/cubicaltt.pdf, https://github.com/mortberg/cubicaltt), a lambda-like syntax can be used to prove propositional equalities (well, paths). The input in this case is a lattice element representing a position along an interval, and the output is the value occurring at that point, defining a full path of intermediate stages between the value at one endpoint and the value at another. Since we're going with the idea that two types have a subtyping relationship in at most one way, we probably don't need to track the content of a subtyping path this comprehensively yet, but it's an interesting precedent to keep in mind.

  - At some point we may like to observe that even if the subtyping relation has an implementation that's uniquely determined by its types, we can still treat monotonic function types as being sugar for a special case of subtyping propositions by using unions and intersections to hide the implementation details:

      (<=<= a : A. B a)
      (A <= B) means (<=<= ignored : A. B)
      FunctionComputeDetails
      FunctionCompute D A
      FunctionComputeResult D A
      FunctionComputeInit X
      (** a : A. B a) means (|| da : FunctionComputeDetails. || db : FunctionComputeDetails. <=<= a : FunctionCompute da A. FunctionCompute db (B (FunctionComputeResult da a)))
      (F X) means (^^ db : FunctionComputeDetails. FunctionComputeResult db (F (FunctionComputeInit X)))

    However, monotonic functions are so intrinsic to the way this type theory deals with its context formation and binder syntaxes that it's going to be easier to treat them as being built in.

    One place this would become tempting again is when pursuing meaning-preserving modularity. Instead of using all the rules of union types (which are weak existentials), we'll want union types to be sugar for exporting and importing values under obscure names. Then another module with access to the author's secrets can come along and say something else about the same obscure names as a way to safely extend the interface of the existing module. Still, in that case we might decide functions are sugar for something simpler, like this very rough sketch:
    
      (**+ a : A. B a) which is the singleton type "Function taking any input `a` of type `A` to the result of expression (B a)."
      FormulaReturning A
      Interpret A
      (** a : A. B a) means (**+ a : A. || b : FormulaReturning (B a). Interpret b)

For dependent typing, we're going to want to be able to use type constructors inside a formula that should be lattice-structure-preserving in its type variables. Certain type constructors, such as function types, are usually contravariant and hence not monotone, and hence not lattice-structure preserving. However, we treat them as being monotone in an input type that's not the usual kind of type universe, but a dual thereof. Then, for every type constructor, we can have a corresponding dual-type constructor.

That will be a lot of redundant constructors, so first, let's draw up a syntax where the type constructors *don't* have to be covariant:

Let: (Let x = A, y = B in C x y)
Least pre-fixed point type: (Fix| x. A x)
Greatest post-fixed point type: (Fix^ x. A x)
Unit type: 1
Unit value introduction: Unit
Boolean type: 2
Boolean value introduction: True and False
Boolean value elimination: (If C A B), with (IsTrue C) meaning (If C 1 Bot)
Bottom type: Bot
Bottom value elimination: Absurd B
Subtyping proposition: (A <= B). Note that 1 is isomorphic to (2 <= 2), along with many other things.
Indexed profinite distributive lattice product type: (** a : A. B a), with (A * B) meaning (** i : 2. If i A B), (A -> B) meaning (** a : A. B), and  (**: A. B) also meaning (** a : A. B)
Indexed profinite distributive lattice product value introduction: (x \* A x)
Indexed profinite distributive lattice product value elimination: (F X)
Indexed profinite distributive lattice sum type: (++ a : A. B a), with (A + B) meaning (++ i : 2. If i A B). Note that Bot is isomorphic to (++ a : Bot. B a) and (A * B) is isomorphic to (++ a : A. B).
Indexed profinite distributive lattice sum value introduction: (A \+ B)
Indexed profinite distributive lattice sum value elimination: (Fst AB) and (Snd AB)
Indexed subtyping sum (join/union) type: (|| a : A. B a), with (A | B) meaning (|| i : 2. If i A B). Note that Bot is isomorphic to (|| a : Bot. B a).
Indexed subtyping product (meet/intersection) type: (^^ a : A. B a), with (A ^ B) meaning (^^ i : 2. If i A B) and Top meaning (^^ a : Bot. B a)
Type of types (if in an impredicative system): Type
The next higher/lower type universe (if in a predicative system): TypeNext U


Now we can systematically define a set of fully covariant type constructors and their fully covariant duals, by using the same syntaxes and adding negation as part of the syntax wherever it's needed:

Let x = A, y = B in C x y
  (self-dual)
Fix| x. A x
Fix^ x. A x
  (duals with each other)
1
-1
Unit
-Unit
2
-2
True
-True
False
-False
If C A B
-If -C -A -B
Bot
-Bot
Absurd B
-Absurd -B
-A <= B
-(A <= -B)
** a : -A. B a
-(** a : A. -(B a))
x \* A x
-(x \* -(A x))
F X
-(-F -X)
++ a : A. B a
-(++ a : -A. -(B a))
A \+ B
-(-A \+ -B)
Fst AB
-Fst -AB
Snd AB
-Snd -AB
|| a : A. B a
^^ a : -A. B a
  (duals with each other)
Type
-Type
TypeNext U
-TypeNext -U

We do likewise for all the abbreviations, such as (-a -> b) and its dual -(a -> -b).

We don't actually define negation as an operation of its own; it's just something that makes the syntax easier to read.

Some of these negative types might coincide nicely with positive types. In particular, we've identified the fixpoints and the union/intersection types as duals of each other because those types only come and go due to type inference and explicit type ascriptions, where the subtyping relations between "negative types" and "positive types" don't need to be segregated. It's tempting to compare this choice to MLsub's polar types, which have the property that unions are only allowed in positive positions and intersections are only allowed in negative ones.

When regarding those type constructors as functions, many of their types can be simply `Type`, (--Type -> --Type -> Type), (** x : --Type. --(x -> --Type) -> Type), or perhaps variations thereof that take multiple type universes into account. For instance, if we had a cumulative type hierarchy, we might like a binary type constructor to have a family of types like (--U -> --V -> (U | V)), for type universes U and V and their negations -U and -V. A binder is usually of type (** x : --U. --(** a : x. --(V x a)) -> U | || a : x. V x a) for every type universe U and its negation -U and every family of type universes (V x a) and their negations -(V x a). (For some binders that expect a dual-type, the first (x : --U) may be (x : -U) instead.)

So far, everything we've done has respected covariance of its bound variables. However, technically we have the flexibility to use bound variables in contravariant ways, which in particular lets us express the usual kind of function polymorphism.

A traditional type system would type the polymorphic identity function like so:

  id : || a : Type. a -> a
  id x = x

However, that uses the variable `a` both covariantly and contravariantly, and it uses the partially contravariant type constructor (a -> b) which we don't have in our system. We need to use the type constructor (-a -> b), so we need access to a negative counterpart to `a` for use in the function's domain. Fortunately, we can get access to that negative counterpart by being polymorphic over another variable and a subtyping proposition between the two:

  id : || pa : Type. || na : -Type. || ignored : (-na <= pa). -na -> pa
  id x = x

This type signature reads "For all types pa, for all dual-types na, for all proofs that -na is a subtype of pa, here's a monotonic function from -na to pa." And indeed, if -na is a subtype of pa, it makes sense we could write an identity function from one type to the other, because the only thing our identity function needs to do is coerce, and we have the coercion by assumption.

That pattern will come in handy a lot. We can abbreviate it:

(~~ nx, sx = -X : A. B nx sx) means (|| nx : A. || sx : (-nx <= X). B nx sx)
(~~ nx = -X : A. B nx sx) means (~~ nx, ignored = -X : A. B nx)

(** px, nx, sx : -NA. B px nx sx) means (** px : -NA. ~~ nx, sx = -px : NA. B px nx sx)
(** px, nx : -NA. B px nx) means (** px, nx, ignored : -NA. B px nx)
(^^ px, nx, sx : -NA. B px nx sx) means (^^ px : -NA. ~~ nx, sx = -px : NA. B px nx sx)
(^^ px, nx : -NA. B px nx) means (^^ px, nx, ignored : -NA. B px nx)
(++ px, nx, sx : PA, NA. B px nx sx) means (++ px : PA. ~~ nx, sx = -px : NA. B px nx sx)
(++ px, nx : PA, NA. B px nx) means (++ px, nx, ignored : PA, NA. B px nx)
(|| px, nx, sx : PA, NA. B px nx sx) means (|| px : PA. ~~ nx, sx = -px : NA. B px nx sx)
(|| px, nx : PA, NA. B px nx) means (|| px, nx, ignored : PA, NA. B px nx)

But the covariant binders require so much duplication of code for this (to fill in both the NA and PA slots with complementary types) that we might as well go further and use the abbreviations (+## ...) and (-## ...) to use a non-polar expression in a polar context and the abbreviations (##+ ...) and (##- ...) to use a polar expression in a non-polar context. If you like, imagine that in a non-polar expression, instead of having each operator build a single expression, we have it build a data structure of two expressions and a subtyping proposition between them. Then we access just one side of that data structure on our way out.

We can define the equality proposition (A = B) as syntax sugar using the same technique. Up to isomorphism, we can have (A = B) stand for (~~ na = -A : -Type. ~~ nb = -B : -Type. ((-na <= B) * (-nb <= A))), or more conveniently (+## ((A <= B) * (B <= A))).

The universe-conscious type of the binder (^^ a : -A. B a) can be (** x : -U. --(** a : x. --(V x a)) -> U | || a : x. V x a), as described above. However, it may also be more precise, at least when the intersection is nonempty, if we use an intersection in the last part instead of a union: (** x : -U. --(** a : x. --(V x a)) -> U | ^^ a : x. V x a).

Our approach to type universes so far has been naive enough to risk Girard's paradox. Since every type is a subtype of `Top`, we know that if `Top` has any type, then (Top : Top), which probably leads to a contradiction via Girard's paradox. But we can still avoid (Top : Top) the way other type systems do, by using a hierarchy of universes. Let's say a top type is only the top of the types of one particular universe, and within that universe, it's not an inhabitant of any type. Only in a strictly bigger universe does that type inhabit another type, and in that universe it's not the top type anymore. So we're dealing with a hierarchy of universes where each type universe in the hierarchy is bounded, but the metatheory we discuss the universe hierarchy in is unbounded (or only bounded by a top corresponding to an infinite ordinal larger than the depth of the hierarchy we're discussing). So we can understand the notations `Top` and `Bot` to be universe-polymorphic, just as type constructors like (** a : A. B a) are. Where the universe is not easily inferred, we should write (^^ t : -NU. t) for `Bot` and (|| t : U. t) for `Top`.

A concrete use case for this idea is that if we want a programmer to be able to define new types, the meaning of `Bot` and `Top` in the *antedated code* the programmer uses may intuitively differ, in some way, from the meaning of `Bot` and `Top` in the *postdated code* that uses the programmer's work. Universe polymorphism accounts for this difference.

Since all our functions are lattice morphisms, they take a top value to a top value and a bottom value to a bottom value. So how can we map the top type of one universe to a non-top type in another universe? Because the `Top` and `Bot` that we obtain from a type universe's empty intersections and unions aren't the same as its actual top and bottom. A type universe has a sublattice of the form L^TB for some lattice L, and only L resembles the overall type system. This means that we finally have an example of a type which is not the free algebra over a set of independent generators. So when we define a function whose domain is a type universe (i.e. it takes a type as input), we must specify not only where to take each coinductively constructed type but also where to take its `Top`, its `Bot`, potentially where to take any other noncanonical representations of its top and bottom types, and proofs that all these extra results have proper interactions with respect to meets and joins.

Fortunately, we can accomplish most of this by construction if we impose some structure on the shape of a type universe again. Let's say that all the non-canonical top values of a universe are in a descending well-ordered chain, with `Top` being the largest non-canonical top value of the universe. Likewise, all the non-canonical bottom values are in an ascending well-ordered chain, and `Bot` is the smallest. Then the way we get a universe's next-bigger universe is clear: We insert a new `Top` between the original universe's `Top` and its canonical top, and we insert a new `Bot` in the dual way. This does not mean that every valid morphism between type universes must respect this convention; some may insert or delete elements of these chains at positions other than the one we want. However, what we can do is provide only a function *syntax* that specifically does what we want for all these non-canonical top and bottom elements, while doing what the programmer specifies for everything else. Someday, we may add another syntax that gives the programmer more flexibility if needed.

(Before I arrived at that approach to the type universe hierarchy, I started by exploring another possible hierarchy: The MLsub type system requires the type (B a) in (Fix| a. B a) to use `a` only in a guarded way, in particular by using a type constructor other than fixpoint, union, or intersection before any use of `a`. This is because union and intersection are unlimited operations of each finite lattice on the way to the final profinite lattice at the limit, so they don't represent progress toward the limit. If in our type system we disallow the literal use of `Bot` and `Top` and require the bodies of (Fix| a. B a), (Fix^ a. B a), (|| a : A. B a), and (^^ a : -A. B a) to be guarded, we might guarantee that every type we deal with has a constructor that guarantees that its *values* make progress too. To model a type constructors like (|| a : A. B a) as a dependently typed functions, we'll want to enforce this guardedness in types, not syntax, so we would require a type to be "guarded" in our system by requiring it to be part of a higher type universe, with each universe's types having at least one more layer of type constructors than the last. This approach may still have promise, although it gets weird when we consider types *isomorphic* to `Bot`, which we'll want at least for representing provably false propositions. Consider (^^ i : FakeBot. 2 -> i) for any `Bot`-isomorphic type `FakeBot`. In that type, the place where `i` occurs doesn't have any particular type constructor, so we're not allowed to specify the type that results when this function is called. That doesn't seem right, maybe bumping the depth by one type constructor isn't enough. If we're allowing types *isomorphic* to `Bot` and `Top`, it's questionable why we wouldn't just distinguish `Bot` and `Top` as being values isomorphic to but not equal to the actual bottom and top types, and that line of questioning led us back to the simpler type universe hierarchy described above.)

Something else that could simplify the matter of functions computing on types is type parametricity. That is, there aren't any type eliminators. If a user has access to a type, they won't do anything with it except insert it into some deeper type or convert it to its counterpart in a larger type universe. However, we don't rely on type parametricity for anything yet, so perhaps we could add type eliminators if we want to for some reason. We'll probably only want to add type eliminators for specific new type constructors that correspond to them, since most of the types we've dealt with (such as unions, intersections, products, and sums) are justified by algebraic properties that should make them unique up to isomorphism. If we add the ability to inspect the exact construction of a type, it will no longer be isomorphic with all the things it was isomorphic with before.

As we think about how user-defined types will work in this system, they should effectively contribute to one giant coinductive-corecursive definition of the type of types. We'll probably want this to be general enough to include inductive-recursive definitions, higher inductive types, and inductive families. In general, when defining new type constructors, type eliminators, and type inequalities, we might need to take care to re-prove basic things about the type system to show that these are a conservative extension of it. On the other hand, if we want one of the basic things known about the type system to be that all its large enough proofs about a small enough prefix of itself are consistent, then we might want to have weaker notions in place of induction/coinduction; see weak arithmetics. Perhaps we can enjoy both styles of reasoning at once by forcing inductive definitions into a much higher type universe (i.e. an infinite ordinal higher) and permitting self-consistency knowledge only to universes that are small enough.

Based on how Cene's macro system works, I imagine a module is a monotonic function from (<a supply of names>, <a definition state which is free over an unknown set of different names>) to <a definition state which is an extension of the original and is free over the union of the name supplies>. The functions corresponding to type constructor syntaxes like (|| a : A. B a) are exports of the original definition state, and we can take advantage of these do some preprocessing in a Cene-like macro system that manually manipulates untyped syntax before interpreting it as typed code. Ordinary definitions on the definition state are comprised of a typed value and a finite-depth serializable type it can be ascribed to.



\= ===== An attempt at describing the type of a module ===============


+##
** typeNil : Kind.
** typeNext : (Kind -> Kind).
** union :
  ( || u : typeNil.
    || vRest : typeNil.
    ** x : u.
    || v : (x -> typeNext vRest).
    **: (** a : x. v a).
    (u | || a : x. v a)).
\= That type again with fully polarized types:
\=
\= ##+
\= ** union :
\=   -( ^^ u, nu : -ntypeNil.
\=      ^^ vRest, nvRest : -ntypeNil.
\=      -** x, nx : u, nu.
\=      -^^ v, nv : --(x -> --(-ntypeNext -nvRest)).
\=      -**: (** a : -nx. v a).
\=      -(nu ^ ^^ a, na : -nx. -(-nv -na))).
\= +##
...
(TODO: Add the rest of the syntaxes as arguments here.)




\= ===== An attempt at inductive and coinductive types ===============


We can build inductive and coinductive definitions in the following style:

(** elem : --U.
  ++ list, nlist : U, -U.
  ** p, np : --(list -> --U).
  **:
    --(** goodList, ngoodList : U, -U.
        --**: (-ngoodList <= list).
        --**: (** x : -ngoodList. p x).
        --** x, nx : (1 + (elem * goodList)), -(-1 + -(-elem * -goodList)).
        --(-np -nx)).
  (** x : -nlist. p x))

(** elem, nelem : --U.
  ++ stream, nstream : U, -U.
  ** p, np : --(stream -> --U).
  **:
    --(** goodStream, ngoodStream : U, -U.
        --**: (-nstream <= goodStream).
        --**: (** x : --(--1 + --(-nelem * -ngoodStream)). p x).
        --** x, nx : goodStream, ngoodStream. --(-np -nx)).
  (** x : -nstream. p x))

This suggests the following abbreviations:

(Ind a : U, NU. B a, NB a) means
  (++ a, na : U, NU.
    ** p, np : --(a -> -NU).
    **:
      --(** goodA, ngoodA : U, NU.
          --**: (-ngoodA <= a).
          --**: (** x : -ngoodA. p x).
          --** x, nx : B goodA, NB goodA. --(-np -nx)).
    (** x : -na. p x))
  aka
  +##
  ++ a : U.
  ** p : (a -> U).
  **:
    (** goodA : U. **: (goodA <= a).
      (** x : goodA. p x) ->
      (** x : B goodA. p x)).
  (** x : a. p x)

(Coind a : U, NU. NB a) means
  (++ a, na : U, NU.
    ** p, np : --(a -> -NU).
    **:
      --(** goodA, ngoodA : U.
          --**: (-na <= goodA).
          --**: (** x : -(NB goodA). p x).
          --** x, nx : goodA, ngoodA. --(-np -nx)).
    (** x : -na. p x))
  aka
  +##
  ++ a : U.
  ** p : (a -> U).
  **:
    (** goodA : U. **: (a <= goodA).
      (** x : ##- (NB goodA). p x) ->
      (** x : goodA. p x)).
  (** x : a. p x)

(TODO: Also explore induction-recursion and coinduction-corecursion.)



(TODO: The rest of the sections below don't account for recent changes we've made to the above syntax. In particular, we didn't write the below with awareness of polarized types. Make changes to the following sections to bring them up to date. The sequent calculus will be much easier to specify now that we depend on every variable monotonically instead of trying to write lambdas to define subtyping propositions.)



\= ===== An attempt at presenting this system with rewrite rules =====


Here are some laws these primitives might follow:

^^ a : A. B
=
B

^^ a : A. B a
<=
B x

^^ a : A. ^^ b : B. C a b
<=
^^ b : B. ^^ a : A. C a b

^^ a : A. ^^ b : B a. C a b
=
^^ ab : (++ a : A. B a). C (Fst ab) (Snd ab)

^^ a : A. ^^ b : B a. C b
=
^^ b : (|| a : A. B a). C b

The dual of that last law also uses || in the type of `b`, not ^^:

|| b : (|| a : A. B a). C b
=
|| a : A. || b : B a. C b


The distributive law for ^ over | (and dually for | over ^) involves the use of a choice function:

^^ a : A. || b : B a. C a b
<=
|| bf : (** a : A. B a). ^^ a : A. C a (bf a)

(TODO: The choice function should probably be intuitionistic, not monotonic (as we have it now).)

Note that the choice function is needed even in the independent case:

^^ a : A. || b : B. C a b
<=
|| bf : (A -> B). ^^ a : A. C a (bf a)

The need for this can be seen in particular when the `a` and `b` indexes are the boolean type. In this case, the distributive law results in a four-way | rather than merely a two-way one:

(C False False | C False True) ^ (C True False | C True True)
<=
(C False False ^ C True False) | (C False False ^ C True True) | (C False True ^ C True False) | (C False True ^ C True True)


(TODO: Make sure we have enough laws to prove commutativity, associativity, and absorption of the lattice operations.)



\= ===== An attempt at presenting this system with sequents ==========


(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases |- b : B
---
env; mono; aliases |- b : ^^ a : A. B

env; mono; aliases |- b : || a : A. B
---
(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases |- b : B

env; mono, a : A; aliases, A |- b : B
---
env; mono; aliases |- a \<= b : <=<= a : A. B

env; mono; <=<= a : A. B |- f : <=<= a : A. B
env; mono; aliases, C |- a2 : A
---
env; mono; aliases, C |- Call<= f a2 : B[a2/a]

env; mono; aliases |- f : <=<= a : A. B
---
env; mono; aliases |- a2 \<= Call<= f a2 : <=<= a : A. B

env; mono; aliases, A |- b : B
(TODO: See if (a : A) should be part of the monotonic environment here.)
env, a : A; mono; aliases, C |- d : D
---
env; mono; aliases, ++ a : A. C |- b \+ d : ++ a : B. D

env; mono; aliases, ++ a : A. B |- c : ++ d : D. E
---
env; mono; aliases, A |- Fst c : D

env; mono; aliases, ++ a : A. B |- c : ++ d : D. E
---
env; mono; aliases, B[(Fst c)/a] |- Snd c : E[(Fst c)/d]
